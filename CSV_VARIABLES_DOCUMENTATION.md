# CSV Variables Documentation

This document describes all variables saved in the CSV files generated by the Social Recognition Memory Task. It reflects the structure of the actual CSV output files.

## File Structure

The experiment generates three CSV files:
1. **recognition_study_[participant_id]_[timestamp].csv** - Study phase data
2. **recognition_trials_[participant_id]_[timestamp].csv** - Recognition phase data
3. **recognition_summary_[participant_id]_[timestamp].csv** - Experiment summary (total time)

The localizer task generates one CSV file:
4. **localizer_[participant_id]_[timestamp].csv** - Localizer task data

**File saving locations**:
- All files are saved to `../LOG_FILES/` directory (created automatically if it doesn't exist)
- File saving is skipped if "test" (case-insensitive) is in the participant name

**CSV format notes**:
- Empty cells indicate `None`/missing values
- `image_path` format may vary by OS (Windows: backslashes; macOS/Linux: forward slashes)
- Column order may vary; use headers to identify columns

---

## Study Phase CSV Variables

**Columns (recognition_study)**: `block`, `phase`, `trial`, `image_path`, `image_onset`, `image_offset`, `image_duration`, `fixation_onset`, `fixation_offset`, `fixation_duration`

---

### `block`
- **Type**: Integer
- **Description**: Block number (0 = practice block, 1-10 = experimental blocks)
- **Example**: `0`, `1`, `2`, `10`

### `phase`
- **Type**: String
- **Description**: Phase identifier, always "study" for study phase data
- **Example**: `"study"`

### `trial`
- **Type**: Integer
- **Description**: Trial number within the study phase (1-indexed)
- **Example**: `1`, `2`, `3`

### `image_path`
- **Type**: String
- **Description**: Full path to the studied image file (always IMAGE_X.png, never lures)
- **Example**: `"PLACEHOLDERS/IMAGE_5.png"`

### `image_onset`
- **Type**: Float (Unix timestamp)
- **Description**: Time when the image was displayed (in seconds since epoch)
- **Example**: `1764818171.2572181`

### `image_offset`
- **Type**: Float (Unix timestamp)
- **Description**: Time when the image was removed from display (in seconds since epoch)
- **Example**: `1764818172.2572181`

### `image_duration`
- **Type**: Float (seconds)
- **Description**: Duration the image was displayed (always 1.0 second, fixed - no jitter)
- **Example**: `1.0`

### `fixation_onset`
- **Type**: Float (Unix timestamp)
- **Description**: Time when the fixation cross appeared before this image
- **Note**: Fixation appears before EVERY image, including the first image
- **Example**: `1764818170.5`

### `fixation_offset`
- **Type**: Float (Unix timestamp)
- **Description**: Time when the fixation cross was removed
- **Example**: `1764818171.0`

### `fixation_duration`
- **Type**: Float (seconds)
- **Description**: Duration of the fixation cross before this image
- **Distribution**: Uniform random between 0.25-0.75 seconds (`random.uniform(0.25, 0.75)`)
- **Jitter**: Each fixation duration is independently drawn from uniform distribution
- **Note**: Fixation appears before EVERY image, including the first image
- **Example**: `0.523456789`, `0.312345`, `0.678901`

---

## Recognition Phase CSV Variables

**Columns (recognition_trials)** — 43 variables: `ai_correct`, `ai_decision_time`, `ai_final_slider_display_time`, `ai_reliability`, `ai_rt`, `ai_slider_display_time`, `ai_slider_value`, `block`, `block_duration_minutes`, `block_duration_seconds`, `block_end_time`, `block_start_time`, `decision_onset_time`, `euclidean_ai_to_truth`, `euclidean_participant_to_ai`, `euclidean_participant_to_truth`, `final_answer`, `ground_truth`, `image_onset`, `image_path`, `is_studied`, `outcome_time`, `participant_accuracy`, `participant_commit_time`, `participant_first`, `participant_rt`, `participant_slider_click_times`, `participant_slider_decision_onset_time`, `participant_slider_stop_time`, `participant_slider_timeout`, `participant_slider_value`, `phase`, `points_earned`, `switch_commit_time`, `switch_rt`, `switch_stay_decision`, `switch_timeout`, `trial`, `trial_type`, `used_ai_answer`

---

### `block`
- **Type**: Integer
- **Description**: Block number (0 = practice block, 1-10 = experimental blocks)
- **Example**: `0`, `1`, `2`, `10`

### `trial`
- **Type**: Integer
- **Description**: Trial number within the recognition phase (1-indexed)
- **Example**: `1`, `2`, `3`

### `phase`
- **Type**: String
- **Description**: Phase identifier, always "recognition" for recognition phase data
- **Example**: `"recognition"`

### `trial_type`
- **Type**: String
- **Description**: Type of trial - either "studied" (original Image version) or "lure" (Lure version of the same object)
- **Example**: `"studied"`, `"lure"`

### `is_studied`
- **Type**: Boolean
- **Description**: True if this is a studied image (original), False if it's a lure
- **Example**: `True`, `False`

### `image_path`
- **Type**: String
- **Description**: Full path to the image shown (Image_XXX.jpg for studied, Lure_XXX.jpg for lures)
  - **Important**: For lures, this is the lure version of the **same object** that was studied (e.g., if `Image_041.jpg` was studied, the lure would be `Lure_041.jpg`, not a random lure from a different object)
  - **Practice block**: Paths point to PLACEHOLDERS (e.g., `PLACEHOLDERS/IMAGE_1.png`)
  - **Path format**: May vary by OS (backslashes on Windows, forward slashes on macOS/Linux)
- **Example**: `"STIMULI/FRUIT/Apple/Image_041.jpg"`, `"STIMULI/FRUIT/Apple/Lure_041.jpg"`, `"PLACEHOLDERS/IMAGE_1.png"`

### `image_onset`
- **Type**: Float (Unix timestamp)
- **Description**: Time when the recognition image was displayed (in seconds since epoch)
- **Example**: `1764818192.494316`

### `participant_first`
- **Type**: Boolean
- **Description**: True if participant responded first in this trial, False if AI responded first. Turn order is randomized within each block (AI goes first on 5 random trials out of 10 per block).
- **Example**: `True`, `False`

---

## Participant Response Variables

### `participant_slider_value`
- **Type**: Float (0.0 to 1.0)
- **Description**: Participant's confidence rating on the slider
  - Values closer to 0.0 = OLD (studied)
  - Values closer to 1.0 = NEW (lure)
  - 0.5 = center/uncertain
- **Example**: `0.3142361111111111`, `0.7890563378785669`

### `participant_rt`
- **Type**: Float (seconds)
- **Description**: Participant's reaction time from image onset to when they clicked SUBMIT
- **Timeout**: If participant doesn't respond within 7.0 seconds, random answer selected and RT = 7.0
- **Example**: `4.68976616859436`, `2.981760025024414`, `7.0` (timeout)

### `participant_commit_time`
- **Type**: Float (Unix timestamp)
- **Description**: Time when participant clicked the SUBMIT button
- **Example**: `1764818198.3314402`

### `participant_slider_timeout`
- **Type**: Boolean
- **Description**: True if participant timed out (didn't respond within 7.0 seconds)
- **Timeout duration**: 7.0 seconds (fixed, no jitter)
- **Example**: `True`, `False`

### `participant_slider_stop_time`
- **Type**: Float (Unix timestamp) or None
- **Description**: Time when participant clicked on the slider to set their rating (final position). None if they never clicked on the slider.
- **Note**: For touch screens, this may be different from `participant_slider_decision_onset_time` if the participant taps multiple times to adjust their rating.
- **Example**: `1764818195.5`, `None`

### `participant_slider_decision_onset_time`
- **Type**: Float (Unix timestamp) or None
- **Description**: Time when participant first clicked/tapped on the slider bar (decision onset - when they start making their decision). 
  - **Touch screen version**: First time they tap the slider bar (may be different from `participant_slider_stop_time` if they tap multiple times to adjust)
  - **Computer/mouse version**: Same as `participant_slider_stop_time` (when they click the slider, decision onset equals the click time)
- **Example**: `1764818195.2`, `None`

### `participant_slider_click_times`
- **Type**: String (comma-separated timestamps) or empty string
- **Description**: List of all times (Unix timestamps) when participant clicked/tapped on the slider bar. 
  - **Touch screen version**: Records all taps on the slider bar (may be multiple taps as they adjust their rating). The first tap is also recorded in `participant_slider_decision_onset_time`.
  - **Computer/mouse version**: Records when mouse button is pressed on the slider bar (may be multiple presses if clicking multiple times or dragging). Decision onset time equals the first press time.
  - Stored as comma-separated string of timestamps for CSV compatibility (field is quoted in CSV when it contains commas)
  - Empty list represented as empty string if no clicks occurred
- **Example**: `"1764818195.2,1764818195.5,1764818196.1"`, `"1764818195.5"`, `""`

---

## AI/Partner Response Variables

### `ai_slider_value`
- **Type**: Float (0.0 to 1.0) or None
- **Description**: AI/partner's confidence rating on the slider
  - Values closer to 0.0 = OLD (studied)
  - Values closer to 1.0 = NEW (lure)
  - `None` for practice trial 1 (no AI response)
  - **Amy (reliable)**: Confidence correlated with correctness. When correct—0.75–1.0 on correct side (OLD: 0–0.25, NEW: 0.75–1.0); when wrong—0.5–0.75 or 0.25–0.5 depending on wrong side. Uniform random within ranges.
  - **Ben (unreliable)**: Random within chosen category (0–0.25 for OLD, 0.75–1.0 for NEW)—uninformative about correctness
- **Example**: `0.6569413750565093`, `0.3563797294608513`, `None`

### `ai_rt`
- **Type**: Float (seconds)
- **Description**: AI's reaction time (drawn from log-normal distribution, capped at 5 seconds)
- **Distribution**: Log-normal with underlying normal parameters: mu = 0.5, sigma = 0.3
- **Mean RT**: Approximately 1.5-2.5 seconds
- **Maximum RT**: 5.0 seconds (capped)
- **Jitter**: Each trial draws independently from the distribution
- **Formula**: `min(np.random.lognormal(0.5, 0.3), 5.0)`
- **Example**: `2.2821904016769365`, `1.2902461381415058`, `4.5`

### `ai_decision_time`
- **Type**: Float (Unix timestamp) or None
- **Description**: Time when AI internally made its decision (right after make_decision() call)
- **Note**: `None` for practice trial 1 (no AI response)
- **Example**: `1764818198.338135`, `None`

### `ai_slider_display_time`
- **Type**: Float (Unix timestamp) or None
- **Description**: Time when AI's slider handle appears at the final position (when the slider value is visually set)
- **Note**: `None` for practice trial 1 (no AI response)
- **Example**: `1764818200.155891`, `None`

### `ai_final_slider_display_time`
- **Type**: Float (Unix timestamp) or None
- **Description**: Time when AI's submit button was clicked in the animation (visual commit time)
- **Note**: `None` for practice trial 1 (no AI response)
- **Example**: `1764818201.255891`, `None`

### `ai_correct`
- **Type**: Boolean or None
- **Description**: True if AI's decision was correct (matches ground truth), False otherwise
- **Note**: `None` for practice trial 1 (no AI response)
- **Example**: `True`, `False`, `None`

### `ai_reliability`
- **Type**: Float (0.0 to 1.0)
- **Description**: AI's accuracy rate/reliability for this trial
  - `0.9` = Reliable partner (Amy in blocks 1-3 and 6-7) - 90% accurate per block (9/10 trials)
  - `0.4` = Unreliable partner (Ben in blocks 4-5 and 8-10) - 40% accurate per block (4/10 trials)
  - `0.5` = Practice block (50% reliability)
- **Note**: With 10 trials per block, reliable blocks yield 9 correct (90%), unreliable blocks yield 4 correct (40%).
- **Example**: `0.9`, `0.4`, `0.5`

---

## Switch/Stay Decision Variables

### `switch_stay_decision`
- **Type**: String
- **Description**: Participant's decision - "stay" (keep own answer) or "switch" (use partner's answer)
- **Example**: `"stay"`, `"switch"`

### `switch_rt`
- **Type**: Float (seconds) or None
- **Description**: Reaction time from when decision screen appeared to when participant clicked STAY or SWITCH
- **Note**: `None` for practice trials 1 and 2 (no switch/stay decision screen shown)
- **Example**: `2.829475164413452`, `1.4260890483856201`, `None`

### `switch_commit_time`
- **Type**: Float (Unix timestamp)
- **Description**: Time when participant clicked STAY or SWITCH button
- **Example**: `1764818206.231731`

### `switch_timeout`
- **Type**: Boolean or None
- **Description**: True if participant timed out on switch/stay decision (didn't respond within 7.0 seconds)
- **Timeout duration**: 7.0 seconds (fixed, no jitter)
- **Note**: `None` for practice trials 1 and 2 (no switch/stay decision screen shown)
- **Example**: `True`, `False`, `None`

### `decision_onset_time`
- **Type**: Float (Unix timestamp) or None
- **Description**: Time of the first flip when the full switch/stay screen was shown (question, image, scale, markers, and STAY/SWITCH buttons). Logged for both touchscreen and click/mouse input modes.
- **Note**: `None` for practice trials 1 and 2 (no switch/stay decision screen shown)
- **Example**: `1764818203.4`, `None`

---

## Final Answer and Accuracy Variables

### `final_answer`
- **Type**: Float (0.0 to 1.0)
- **Description**: Final answer used for scoring (participant's value if stayed, AI's value if switched)
- **Example**: `0.0`, `0.3563797294608513`

### `used_ai_answer`
- **Type**: Boolean
- **Description**: True if final answer came from AI (participant switched), False if from participant (stayed)
- **Example**: `True`, `False`

### `ground_truth`
- **Type**: Float
- **Description**: Correct answer (0.0 for studied/OLD, 1.0 for lure/NEW)
- **Example**: `0.0`, `1.0`

### `participant_accuracy`
- **Type**: Boolean
- **Description**: True if final answer was correct (within 0.5 of ground truth), False otherwise
- **Example**: `True`, `False`

---

## Distance Metrics

### `euclidean_participant_to_truth`
- **Type**: Float
- **Description**: Euclidean distance between participant's slider value and ground truth
  - Lower values = closer to correct answer
- **Example**: `0.3142361111111111`, `0.0`

### `euclidean_ai_to_truth`
- **Type**: Float or None
- **Description**: Euclidean distance between AI's slider value and ground truth
  - Lower values = closer to correct answer
- **Note**: `None` for practice trial 1 (no AI response)
- **Example**: `0.6569413750565093`, `0.0`, `None`

### `euclidean_participant_to_ai`
- **Type**: Float
- **Description**: Euclidean distance between participant's and AI's slider values
  - Lower values = more similar ratings
- **Example**: `0.6569413750565093`, `0.042143618349740175`

---

## Outcome Variables

### `outcome_time`
- **Type**: Float (Unix timestamp) or None
- **Description**: Time when the outcome screen (Correct/Incorrect) was displayed
- **Note**: `None` for practice trials 1 and 2 (outcome not tracked separately)
- **Example**: `1764818206.237804`, `None`

### `block_start_time`
- **Type**: Float (Unix timestamp)
- **Description**: Time when the block started (when study phase began)
- **Example**: `1764818000.0`

### `block_end_time`
- **Type**: Float (Unix timestamp) or None
- **Description**: Time when the block ended (after block summary screen)
- **Note**: `None` for practice block (block 0) - practice block timing not tracked
- **Example**: `1764818300.0`, `None`

### `block_duration_seconds`
- **Type**: Float (seconds)
- **Description**: Total duration of the block in seconds (from start of study phase to end of block summary)
- **Example**: `300.5`

### `block_duration_minutes`
- **Type**: Float (minutes) or None
- **Description**: Total duration of the block in minutes
- **Note**: `None` for practice block (block 0) - practice block timing not tracked
- **Example**: `5.008`, `None`

### `points_earned`
- **Type**: Float
- **Description**: Points earned this trial (based on Euclidean distance from correct answer)
  - Formula: `1.0 - euclidean_distance(final_answer, ground_truth)`
  - Range: 0.0 to 1.0
  - Based on correctness only (Euclidean distance from correct answer)
  - **Precision**: Full precision is maintained in logged data (not rounded). Display shown to participants is rounded to 1 decimal place for readability.
- **Example**: `0.6857638888888889`, `0.0`, `1.0`

---

## Summary CSV Variables

The **recognition_summary_[participant_id]_[timestamp].csv** file contains overall experiment summary data.

**Columns (recognition_summary)**: `participant_id`, `experiment_start_time`, `experiment_end_time`, `total_task_time_seconds`, `total_task_time_minutes`

---

### `participant_id`
- **Type**: String
- **Description**: Participant identifier
- **Example**: `"kini"`, `"P001"`

### `experiment_start_time`
- **Type**: Float (Unix timestamp)
- **Description**: Time when experiment started (after initial instructions)
- **Example**: `1764818000.0`

### `experiment_end_time`
- **Type**: Float (Unix timestamp)
- **Description**: Time when experiment ended (after final instructions)
- **Example**: `1764820800.0`

### `total_task_time_seconds`
- **Type**: Float (seconds)
- **Description**: Total duration of the experiment in seconds
- **Example**: `2800.5`

### `total_task_time_minutes`
- **Type**: Float (minutes)
- **Description**: Total duration of the experiment in minutes
- **Example**: `46.675`

---

## Notes

- All timestamps are in Unix time (seconds since January 1, 1970)
- All slider values range from 0.0 (OLD/studied) to 1.0 (NEW/lure)
- **Timeout settings (Main Task only)**: Timeout variables are True if the participant didn't respond within the time limit:
  - Slider and switch/stay decisions: **7.0 seconds** (fixed)
  - **Note**: The localizer task uses a different timeout (10.0 seconds for questions) - see Localizer Task section below
- The experiment saves data incrementally after each trial, not just at the end
- Block 0 is the practice block (3 trials), blocks 1-10 are experimental blocks (10 trials each)
- **Narrative context**: The experiment is framed as a photography studio collaboration. It is Amy's shop. Carly is Amy's assistant and appears only in the practice block (same image as Amy). Experimental blocks use Amy (reliable partner) or Ben (unreliable partner) to help sort images for an exhibition. Scoring is framed as "in-house curator" evaluations.
- **Scoring display**: Trial outcomes show "The in-house curator scored this image: X points" instead of "Points earned this trial". Block summaries show "The in-house curator scored this collection X points out of a total of 10 points!" (actual points, not scaled). All point displays (trial, block summary, final score) are rounded to 1 decimal place for readability, but logged data maintains full precision.
- Points are calculated based on Euclidean distance: `points = 1.0 - distance(final_answer, ground_truth)`
- **Practice Block (Block 0)**:
  - Contains 3 practice trials with simplified stimuli (colored shapes)
  - All practice trials include the same fields as regular trials
  - Fields that don't apply to specific practice trials are set to `None` or `False`:
    - Trial 1: No AI response (`ai_slider_value`, `ai_rt`, `ai_decision_time`, `ai_slider_display_time`, `ai_final_slider_display_time`, `ai_correct` all `None`), no switch/stay decision (all switch fields `None`), `euclidean_ai_to_truth` and `euclidean_participant_to_ai` are `None`. `ai_reliability` is `0.5` (50% for practice block)
    - Trial 2: Has AI response, but no switch/stay decision (switch fields `None`). `ai_reliability` is `0.5` (50% for practice block). Partner shown as Carly (Amy's assistant).
    - Trial 3: Full trial with participant, AI, and switch/stay decision. `ai_reliability` is `0.5` (50% for practice block). Partner shown as Carly (Amy's assistant).
  - All practice trials have `block_start_time`, `block_end_time`, `block_duration_seconds`, and `block_duration_minutes` set to `None` (practice block timing not tracked)
- **Block structure**:
  - Blocks 1-3: Reliable (Amy, 90% = 9/10 correct)
  - Blocks 4-5: Unreliable (Ben, 40% = 4/10 correct)
  - Blocks 6-7: Reliable (Amy, 90% = 9/10 correct)
  - Blocks 8-10: Unreliable (Ben, 40% = 4/10 correct)
  
  **AI Accuracy Implementation**: With 10 trials per block:
  - **Reliable blocks (Amy)**: 90% accuracy (9 correct per 10-trial block)
  - **Unreliable blocks (Ben)**: 40% accuracy (4 correct per 10-trial block)
- **Turn-taking**: 
  - Turn order is randomized within each block: AI goes first on a **random 5 out of 10 trials** in each block
  - The 5 trials where AI goes first are randomly selected for each block (different randomization per block)
  - Participant goes first on the remaining 5 trials in each block
  - The `participant_first` field logs who goes first for each trial (True = participant first, False = AI first)
- **Study phase timing**:
  - Images are shown for **1.0 second each** (fixed duration, no jitter)
  - **Jittered fixations** appear before EVERY image: **0.25-0.75 seconds** (uniform random distribution)
  - Fixation jitter: `random.uniform(0.25, 0.75)` - each fixation independently drawn
  - Fixation appears before the first image and between all subsequent images
  - 10 fixations per block (before each of the 10 images)
  - Total study phase duration: ~15-17.5 seconds (varies due to fixation jitter)
- **Recognition phase timing**:
  - **Pre-trial fixation**: 0.5 seconds (fixed duration, shown before each image)
  - Images are shown for **1.0 second each** (fixed duration, no jitter)
    - Image remains visible until participant responds or timeout (7 seconds)
  - **Jittered fixations** appear between trials: **0.25-0.75 seconds** (uniform random distribution)
  - Inter-trial jitter: `random.uniform(0.25, 0.75)` - each jitter independently drawn
  - Jitter shown as fixation cross during the inter-trial interval
  - **No jitter after the last trial** in each block
  - **9 jittered fixations per block** (between 10 trials)
  - Participant slider timeout: **7.0 seconds** (fixed)
  - AI RT: Log-normal distribution (mu=0.5, sigma=0.3), capped at 5.0 seconds
- **Switch/stay decision timing**:
  - Decision timeout: **7.0 seconds** (fixed)
- **Block timing**: `block_start_time`, `block_end_time`, and `block_duration_seconds`/`block_duration_minutes` are added to all trials within a block. These values are updated at the end of each block, so they represent the complete block duration from study phase start to block summary completion.

---

## Localizer Task CSV Variables

The **localizer_[participant_id]_[timestamp].csv** file contains data from the localizer task, where participants view 200 images (100 Image + 100 Lure versions) in random order and answer object questions at every 10th image.

**Columns (localizer)** — 24 variables: `participant_id`, `trial`, `stimulus_number`, `object_name`, `category`, `stimulus_type`, `is_lure`, `image_path`, `presentation_time`, `fixation_onset_time`, `fixation_offset_time`, `fixation_duration`, `image_onset_time`, `image_offset_time`, `is_question_trial`, `question_object`, `question_text`, `question_onset_time`, `answer`, `correct_answer`, `correct`, `timed_out`, `response_time`, `answer_click_time`

**File structure**: Each row represents one image presentation, logged in presentation order (trial 1 = first image shown, trial 2 = second image shown, etc.). All 200 images are logged sequentially, with question response data included for question trials (every 10th trial). Feedback is given at task end only.

**What is recorded**:
- Image metadata: `image_path`, `object_name`, `category`, `stimulus_type`, `stimulus_number`
- Presentation order: `trial` (1-200)
- Fixation timing: `fixation_onset_time`, `fixation_offset_time`, `fixation_duration` (jittered 0.25-0.75 s)
- Image timing: `image_onset_time`, `image_offset_time`
- Question trials (every 10th): `question_object`, `question_text`, `question_onset_time`, `answer`, `correct_answer`, `correct`, `timed_out`, `response_time`, `answer_click_time`

**Important**: The localizer task has different timeout settings than the main task:
- **Localizer question timeout**: 10.0 seconds (fixed)
- **Main task timeouts**: 7.0 seconds for slider and switch/stay decisions

**Stimulus presentation timing**:
- Starts with a **fixation cross** before the first image (jittered 0.25-0.75 seconds)
- **Jittered fixation cross** appears between images: **0.25-0.75 seconds** (uniform random distribution)
- Each fixation duration is independently drawn from `random.uniform(0.25, 0.75)`
- Fixation appears before EVERY image, including the first image
- Image presentation duration is **0.5 seconds** (fixed, no jitter)
- This prevents predictable timing patterns and helps reduce anticipatory responses

### `participant_id`
- **Type**: String
- **Description**: Participant identifier
- **Example**: `"P001"`, `"test_user"`

### `trial`
- **Type**: Integer
- **Description**: Trial number representing the presentation order (1-indexed, 1-200)
- **Presentation order**: Each image is logged in the order it appears (trial 1 = first image shown, trial 2 = second image shown, etc.)
- **Question trials**: Questions are asked at trials 10, 20, 30, ..., 200 (every 10th trial)
- **Example**: `1`, `10`, `20`, `50`, `200`

### `stimulus_number`
- **Type**: Integer (1-100)
- **Description**: The stimulus number of the image that was shown (from Image_001.jpg to Image_100.jpg)
- **Example**: `1`, `42`, `100`

### `object_name`
- **Type**: String
- **Description**: Name of the object folder containing the image (e.g., "Apple", "Car", "Elephant")
- **Example**: `"Apple"`, `"Car"`, `"Elephant"`

### `category`
- **Type**: String
- **Description**: Category folder name that the image belongs to
- **Possible values**: `"BIG_ANIMAL"`, `"BIG_OBJECT"`, `"BIRD"`, `"FOOD"`, `"FRUIT"`, `"INSECT"`, `"SMALL_ANIMAL"`, `"SMALL_OBJECT"`, `"VEGETABLE"`, `"VEHICLE"`
- **Example**: `"FRUIT"`, `"BIG_ANIMAL"`

### `stimulus_type`
- **Type**: String
- **Description**: Type of stimulus shown
- **Possible values**: `"Image"` (original image), `"Lure"` (lure version)
- **Example**: `"Image"`, `"Lure"`

### `is_lure`
- **Type**: Boolean
- **Description**: True if this is a lure stimulus, False if it's an original Image
- **Example**: `True`, `False`

### `image_path`
- **Type**: String
- **Description**: Full path to the image file that was displayed
- **Example**: `"STIMULI/biganimal/Elephant/Image_042.jpg"`, `"STIMULI/smallobject/Key/Lure_042.jpg"`

### `presentation_time`
- **Type**: String (datetime format)
- **Description**: Timestamp when the image was displayed (human-readable format)
- **Format**: `"YYYY-MM-DD HH:MM:SS.ffffff"`
- **Example**: `"2026-01-30 23:21:31.123456"`

### `fixation_onset_time`
- **Type**: Float (Unix timestamp)
- **Description**: Time when the fixation cross appeared before this image (in seconds since epoch, high precision)
- **Note**: Fixation appears before EVERY image, including the first image
- **Example**: `1764818170.5`

### `fixation_offset_time`
- **Type**: Float (Unix timestamp)
- **Description**: Time when the fixation cross was removed (in seconds since epoch, high precision)
- **Example**: `1764818170.75`

### `fixation_duration`
- **Type**: Float (seconds)
- **Description**: Duration the fixation cross was displayed (jittered 0.25-0.75 seconds)
- **Distribution**: `random.uniform(0.25, 0.75)` - each fixation independently drawn
- **Example**: `0.42`, `0.68`, `0.31`

### `image_onset_time`
- **Type**: Float (Unix timestamp)
- **Description**: Time when the image was displayed (in seconds since epoch, high precision)
- **Example**: `1764818171.2572181`

### `image_offset_time`
- **Type**: Float (Unix timestamp)
- **Description**: Time when the image was removed from display (in seconds since epoch, high precision)
- **Note**: Images are displayed for exactly 0.5 seconds, so `image_offset_time - image_onset_time = 0.5`
- **Example**: `1764818171.7572181`

### `is_question_trial`
- **Type**: Boolean
- **Description**: True if this trial included an object question (trials 10, 20, 30, ..., 200), False otherwise
- **Example**: `True`, `False`

### `question_object`
- **Type**: String or None
- **Description**: The object name that was asked about in the question (e.g., "Giraffe", "Elephant")
- **Question design**: 50% of trials ask about the correct object (matches `object_name`); 50% ask about a **random incorrect object** selected from all other objects in the stimulus set. Pre-generated sequence ensures exactly 10 correct and 10 incorrect questions in randomized order.
- **Example**: `"Giraffe"`, `"Elephant"`, `"Apple"` (may or may not match the actual `object_name` of the image)

### `question_text`
- **Type**: String or None
- **Description**: Full text of the question asked to the participant (only populated for question trials)
- **Format**: "Was the last object a [object]?" or "Was the last object an [object]?" where object name is lowercase
- **Article selection**: Uses "a" or "an" based on whether the object starts with a vowel sound
- **Example**: `"Was the last object a giraffe?"`, `"Was the last object an elephant?"`, `"Was the last object an apple?"`, `None`

### `question_onset_time`
- **Type**: Float (Unix timestamp) or None
- **Description**: Time when the question was displayed (in seconds since epoch, high precision)
- **Note**: Only populated for question trials (every 10th trial). `None` for non-question trials.
- **Example**: `1764818180.1234567`, `None`

### `answer`
- **Type**: Boolean, String, or None
- **Description**: Participant's response to the question
- **Possible values**: 
  - `True` (YES) for question trials where participant answered YES
  - `False` (NO) for question trials where participant answered NO
  - `"TIMEOUT"` if participant timed out (10.0 second timeout)
  - Empty/`None` for non-question trials
- **Example**: `True`, `False`, `"TIMEOUT"`, empty

### `correct_answer`
- **Type**: Boolean or None
- **Description**: The correct answer to the question
- **Question design**: 
  - `True` if the question asks about the correct object (matches the actual `object_name` of the image)
  - `False` if the question asks about an incorrect object (does not match the actual `object_name` of the image)
  - `None` for non-question trials
- **Example**: `True`, `False`, `None`

### `correct`
- **Type**: Boolean or None
- **Description**: Whether the participant's answer matches the correct answer
- **Possible values**: `True` (correct), `False` (incorrect), `None` (non-question trial or timeout)
- **Example**: `True`, `False`, `None`

### `timed_out`
- **Type**: Boolean or None
- **Description**: Whether the participant timed out on the question
  - `True` if participant didn't respond within 10.0 seconds
  - `False` if participant responded before timeout
  - `None` for non-question trials
- **Timeout duration**: 10.0 seconds (fixed)
- **Example**: `True`, `False`, `None`

### `response_time`
- **Type**: Float (seconds) or None
- **Description**: Time taken to respond to the question, measured from question onset to button press
- **Example**: `1.234`, `2.567`, `None`

### `answer_click_time`
- **Type**: Float (Unix timestamp) or None
- **Description**: Absolute timestamp when the participant clicked their answer (in seconds since epoch, high precision)
- **Note**: Only populated for question trials where participant answered (YES/NO button click or y/n key press). `None` for non-question trials or if participant timed out.
- **Example**: `1764818181.1234567`, `None`

---

## Notes on Localizer Task

- **Fixation cross timing**:
  - Starts with a fixation cross before the first image (jittered 0.25-0.75 seconds)
  - Jittered fixation appears between images: 0.25-0.75 seconds (uniform random distribution)
  - Fixation jitter: `random.uniform(0.25, 0.75)` - each fixation independently drawn
  - Fixation appears before EVERY image, including the first image
  - 200 fixations total (before each of the 200 images)
- **Image presentation timing**:
  - Each image is displayed for **0.5 seconds** (fixed duration, no jitter)
  - Total images: 200 (100 Image + 100 Lure versions)
  - Total image presentation time: 200 images × 0.5 seconds = 100 seconds (~1.7 minutes)
  - Total fixation time: ~50-150 seconds (varies due to randomization)
  - **Total task duration**: Approximately 3-5 minutes (varies due to fixation jitter and question response times)
- **Question timing**:
  - Questions are asked at trials 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200 (20 questions total)
  - **Timeout**: 10.0 seconds - if participant doesn't respond within 10 seconds, the question times out and the task continues to the next image
  - Question appears immediately after the image is shown
- **Feedback**: No per-trial feedback. Accuracy summary ("Your accuracy: X/20 (Y%)") is shown at the very end of the task only.
- **Exit availability**: ESC and Exit work at all times, including during fixation and image presentation.
- **Question design** (50/50 split, randomized order):
  - **50% correct questions (10 trials)**: Ask about the actual object shown (e.g., "Was the last object a giraffe?") (correct answer = True)
  - **50% incorrect questions (10 trials)**: Ask about a **random incorrect object** drawn from all other objects in the stimulus set (correct answer = False). The incorrect object is randomly selected each trial.
  - Uses pre-generated sequence of exactly 10 True + 10 False, shuffled, to guarantee the 50/50 split
- **Object question format**: "Was the last object a [object]?" or "Was the last object an [object]?" (object name in lowercase, article "a"/"an" based on vowel)
- **File saving**: 
  - Skipped if "test" (case-insensitive) is in the participant name
  - All files are saved to `../LOG_FILES/` directory (created automatically if it doesn't exist)
  - File naming format: `localizer_[participant_id]_[timestamp].csv`
  - Example: `localizer_kini_20260130_232131.csv`

